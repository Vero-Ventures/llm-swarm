{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87dd2cad-d72b-4a38-b7b1-58b2eab74cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to track processing time\n",
    "import time\n",
    "\n",
    "# Used to write a file with the current datetime\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from textwrap import dedent\n",
    "\n",
    "# Used to import LLM and set up mutli-agent crews\n",
    "from langchain_community.llms import Ollama\n",
    "from crewai import Crew, Agent, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d4f3be-53cb-4f3a-9957-b21e2910708f",
   "metadata": {},
   "source": [
    "## Initialize LLM\n",
    "For our VSCode extension and testing, we're using a popular open-sourced LLM called **Llama3**, the eight-billion parameter version. We download and manage these LLMs using [**Ollama**](https://ollama.com), an open-source platform for running LLMs locally.\n",
    "\n",
    "Ollama allows us to create and use custom models. For testing purposes, we're creating a Llama3 model with a set seed. A seed means that the same input will produce the same output. (Seeding, in practice, does not produce exactly the same outputs given a certain input; this is due to several reasons, including stochastic sampling, model parameter size, and so on.)\n",
    "\n",
    "Custom models are stored in the same place, locally, as the default model.\n",
    "\n",
    "### MacOS\n",
    "`/Users/[username]/.ollama/models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "### Windows\n",
    "`C:\\Users\\[username]\\.ollama\\manifests\\registry.ollama.ai\\library\\`\n",
    "\n",
    "### Linux\n",
    "`/var/lib/ollama/.ollama/models/manifests/registry.ollama.ai/library`\n",
    "\n",
    "If you want to initialize a custom model, simply reference the name of the model as you would the default model:\n",
    "\n",
    "```Python\n",
    "llm = Ollama(model=\"[custom_model_name\"])\n",
    "```\n",
    "\n",
    "This will automatically look in the same directory as the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e4f3bc-4866-4999-99da-8bac8d5196d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"Llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4982559-f286-4ebf-ae17-728349d0de71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Swarm Env",
   "language": "python",
   "name": "llm-swarm-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
